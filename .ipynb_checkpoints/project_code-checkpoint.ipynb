{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7e1516-077a-46fd-94a6-6d6a158fa2a7",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393781dc-648b-406e-a010-e628e1ee670a",
   "metadata": {},
   "source": [
    "We will use the Datasets library to download the data and get the metric we need to use for further evaluation between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8edef96-1633-4739-8ff4-99b413a7f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aa5f91-28a0-4cc2-b2ea-102852f837fe",
   "metadata": {},
   "source": [
    "Set a seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaecae12-2286-4aac-ab78-e3d4467e4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 306\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112bd76-353f-40c4-ade6-03ffc00e8beb",
   "metadata": {},
   "source": [
    "We set the default device to CPU so that all models are created on the CPU instead of the MPS/GPU backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe4c81bc-13dd-4c35-a4cf-3b3fcf9000db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e614d-7aec-401b-96f9-90311f77773f",
   "metadata": {},
   "source": [
    "Load the split dair-ai/emotion dataset, which has a total of 20,000 examples split into train, validation and split sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c2b6bd-31c2-4ebe-9c3f-2e20fd516441",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\", \"split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee13968-88eb-4f50-a80c-b47a64481510",
   "metadata": {},
   "source": [
    "We can access the element with its split and index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6ed46d4-3323-4c7d-9467-7feec3762562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i didnt feel humiliated', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301eaf3-ef79-43d6-b22c-705f9e54b313",
   "metadata": {},
   "source": [
    "Extract the names of the six emotion labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d99cf26f-d09e-4394-839d-360f156cb896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label names: ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "emotion_labels = dataset[\"train\"].features[\"label\"].names\n",
    "print(\"Label names:\", emotion_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fcc4fa-f0c9-415d-888d-470e4a4b97c7",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804cb7e3-6239-46f9-91f4-e3ccadad64eb",
   "metadata": {},
   "source": [
    "Set our model checkpoint and load its tokenizer. Then, define a tokenize_function and apply it to the whole dataset in batched mode for efficiency. Dynamic padding pads each batch only to the length of its longest sequence, making training faster and more efficient than padding everything to a fixed maximum length. Because 99% of texts have fewer than 57 tokens, using a max length of 64 is sufficient and avoids unnecessary memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fa47e1c-ca81-4773-a5d5-9553de657a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7243c5f30bac4f92ac9b22c22c717e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea3490267c64c86be0d40213ee34bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4447b2811916465f8a996893c649707a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"prajjwal1/bert-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=64)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9ed4b77-fad9-4438-9aa7-e48953a10f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 87\n",
      "Mean length: 22.2595\n",
      "99th percentile: 57.0\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for text in dataset[\"train\"][\"text\"]:\n",
    "    tokens = tokenizer(text, truncation=False, add_special_tokens=True)\n",
    "    lengths.append(len(tokens[\"input_ids\"]))\n",
    "\n",
    "print(\"Max length:\", np.max(lengths))\n",
    "print(\"Mean length:\", np.mean(lengths))\n",
    "print(\"99th percentile:\", np.percentile(lengths, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cabdfb22-7d38-4e9a-8d1f-8f4d5cc8540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb55f8-73f4-4bb3-b074-0b91b79d0627",
   "metadata": {},
   "source": [
    "We remove the raw text column, rename the label column to â€˜labelsâ€™, and set the dataset format to PyTorch tensors to ensure compatibility with the Trainer API and maintain a consistent input structure across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59b7c318-0787-486b-b2f6-35b916002bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(tokenized_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e163a3e-438a-43b3-9a2e-13aaabda3c8c",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81845117-b49b-4713-bf4b-c9804ee088e0",
   "metadata": {},
   "source": [
    "We can now use the preprocessed data to fine-tune the model. We use the AutoModelForSequenceClassification class for emotion classifications. There are 6 labels because the emotion dataset has six emotion categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59d672f0-d2a1-4a04-8cf9-1f85488a6878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef6146-f7c1-43e7-8049-a53cb82eda2a",
   "metadata": {},
   "source": [
    "Define a compute_metrics function that converts the model logits into predicted class labels using the argmax operation, compares these predictions with the true labels, and returns evaluation metrics such as accuracy, macro precision, macro recall, and macro F1-score. We use macro averaging treats all classes equally, because our emotion dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2caf5e4f-7d18-4a34-ae8f-efaac6119cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_precision\": precision,\n",
    "        \"macro_recall\": recall,\n",
    "        \"macro_f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ccfe0-0cbc-4eae-b481-1130ff2e2efd",
   "metadata": {},
   "source": [
    "Set up the TrainingArguments to specify where model checkpoints are saved and evaluation should be made at the end of each epoch. We also set the batch size to 8 to ensure stable training on a CPU-based environment, which can be adjusted later. Since the last model in the training might not be the best one, we let the Trainer load the best model at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a1295cf-9e5a-497f-a3e3-17987a69a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned\", # Where to save checkpoints\n",
    "    eval_strategy=\"epoch\",                # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                # Save a checkpoint each epoch\n",
    "    learning_rate=2e-5,                   # A standard fine-tuning learning rate\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,                  \n",
    "    weight_decay=0.01,                    # Slight regularization\n",
    "    load_best_model_at_end=True,          # Restore best model (on validation set)\n",
    "    metric_for_best_model=\"macro_f1\",     # Use macro F1 to select the best checkpoint\n",
    "    greater_is_better=True,               # Higher macro F1 is better\n",
    "    use_cpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29584949-2872-423c-9118-926dee44c66c",
   "metadata": {},
   "source": [
    "Pass the model, training arguments, training and test datasets, and evaluation function to the trainer we have created. Then we can call train() to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46a7b1fe-658c-4229-899f-fb120b5cb1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 09:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>0.252904</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.877808</td>\n",
       "      <td>0.905361</td>\n",
       "      <td>0.889635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>0.202321</td>\n",
       "      <td>0.931500</td>\n",
       "      <td>0.900660</td>\n",
       "      <td>0.909096</td>\n",
       "      <td>0.904687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.159200</td>\n",
       "      <td>0.208344</td>\n",
       "      <td>0.935500</td>\n",
       "      <td>0.910749</td>\n",
       "      <td>0.909160</td>\n",
       "      <td>0.909150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.32123424784342447, metrics={'train_runtime': 565.4055, 'train_samples_per_second': 84.895, 'train_steps_per_second': 10.612, 'total_flos': 152955539229312.0, 'train_loss': 0.32123424784342447, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator, \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c31dbbb-c1fb-41b3-b560-7e0bb4ea6680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24242918193340302,\n",
       " 'eval_accuracy': 0.9205,\n",
       " 'eval_macro_precision': 0.872061240911096,\n",
       " 'eval_macro_recall': 0.8696265739494918,\n",
       " 'eval_macro_f1': 0.8703872097839622,\n",
       " 'eval_runtime': 4.9691,\n",
       " 'eval_samples_per_second': 402.483,\n",
       " 'eval_steps_per_second': 50.31,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb90fda-7f29-47b4-ba5b-c840095b2844",
   "metadata": {},
   "source": [
    "# Evaluation and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803198e1-d906-445a-81ce-675fbf81ef10",
   "metadata": {},
   "source": [
    "Compute per-class F1-scores to identify which emotions are particularly challenging for smaller models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cb7f043-8ac2-4e84-8c86-7988cbb8beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(tokenized_datasets[\"test\"])\n",
    "y_true = pred.label_ids\n",
    "y_pred = pred.predictions.argmax(axis=-1)\n",
    "\n",
    "per_class_f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "f1_df = pd.DataFrame({\n",
    "    \"Emotion\": emotion_labels,\n",
    "    \"F1-score\": per_class_f1\n",
    "})\n",
    "f1_df = f1_df.sort_values(\"F1-score\", ascending=False).reset_index(drop=True)\n",
    "f1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97cac42-866d-4605-a663-430c92db6b51",
   "metadata": {},
   "source": [
    "Draw a confusion matrix heatmap using seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b36292b-834d-4159-99e6-af70ae5e0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=emotion_labels, columns=emotion_labels)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(\n",
    "    cm_df, \n",
    "    annot=True, \n",
    "    fmt=\"d\", \n",
    "    cmap=\"Oranges\",\n",
    "    linewidths=.5, \n",
    "    cbar=True\n",
    ")\n",
    "plt.title(\"Confusion Matrix for BERT-Tiny Model\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.savefig(\"confusion_matrix_berttiny.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
